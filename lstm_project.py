# -*- coding: utf-8 -*-
"""LSTM.project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11FtAilHY-NwgliY1GYgCK1zcYIddbE5o
"""

import bz2
from tqdm import tqdm
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score,recall_score,precision_score ,accuracy_score
from wordcloud import WordCloud
from sklearn.metrics import confusion_matrix,classification_report
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,LSTM,SpatialDropout1D,Embedding
from keras.callbacks import ModelCheckpoint
import pickle

with open('/content/train.ft.txt', 'r', encoding='utf-8') as file:
    train_lines = file.readlines()

with open('/content/test.ft.txt', 'r', encoding='utf-8') as file:
    test_lines = file.readlines()

train_texts, train_labels = [], []
for line in train_lines:
    parts = line.split('__label__')
    train_labels.append(parts[1][0])
    train_texts.append(parts[1][1:])

test_texts, test_labels = [], []
for line in test_lines:
    parts = line.split('__label__')
    test_labels.append(parts[1][0])
    test_texts.append(parts[1][1:])

def clean_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text.lower()

cleaned_train_texts = [clean_text(text) for text in train_texts]
cleaned_test_texts = [clean_text(text) for text in test_texts]

train_labels_df = pd.DataFrame(train_labels, columns=['label'])
test_labels_df = pd.DataFrame(test_labels, columns=['label'])

sns.countplot(x='label', data=train_labels_df)
plt.title('Distribution of Training Labels')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

sns.countplot(x='label', data=test_labels_df, )
plt.title('Distribution of Test Labels')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

tokenizer = Tokenizer(num_words=20000)
tokenizer.fit_on_texts(cleaned_train_texts)

train_sequences = tokenizer.texts_to_sequences(cleaned_train_texts)
test_sequences = tokenizer.texts_to_sequences(cleaned_test_texts)

train_padded = pad_sequences(train_sequences, maxlen=100)
test_padded = pad_sequences(test_sequences, maxlen=100)

train_labels_binary = np.array([1 if label == '2' else 0 for label in train_labels])
test_labels_binary = np.array([1 if label == '2' else 0 for label in test_labels])

model = Sequential()
model.add(Embedding(input_dim=20000, output_dim=64, input_length=100))
model.add(LSTM(64, return_sequences=True))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(64))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

history = model.fit(train_padded, train_labels_binary, epochs=5, validation_split=0.1, batch_size=64)

predictions = (model.predict(test_padded) > 0.5).astype("int32")

acc =accuracy_score(test_labels_binary, predictions)
print('Accuracy: ',acc*100)

f1 = f1_score(test_labels_binary, predictions)
print('F1 Score: ',f1*100)

recall = recall_score(test_labels_binary, predictions)
print('Recall: ',recall*100)

precision = precision_score(test_labels_binary, predictions)
print('Precision: ',precision*100)

cm = confusion_matrix(test_labels_binary, predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])

new_text = "This movie is terrible, I hated it."
new_text = clean_text(new_text)
new_sequence = tokenizer.texts_to_sequences([new_text])
new_padded = pad_sequences(new_sequence, maxlen=100)
prediction = model.predict(new_padded)
if prediction > 0.5:
  print("Positive sentiment")
else:
  print("Negative sentiment")

new_text = "The movie was amazing! I loved every minute of it."
new_text = clean_text(new_text)
new_sequence = tokenizer.texts_to_sequences([new_text])
new_padded = pad_sequences(new_sequence, maxlen=100)
prediction = model.predict(new_padded)
if prediction > 0.5:
  print("Positive sentiment")
else:
  print("Negative sentiment")